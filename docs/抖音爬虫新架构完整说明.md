# 抖音爬虫新架构 - 完整说明文档

**版本**: v2.0
**创建日期**: 2025-10-27
**状态**: ✅ 基础架构完成

---

## 📁 文件结构

```
packages/worker/src/platforms/douyin/
├── _backup_20251027/                    # ✅ 旧代码备份
│   ├── platform.js                      # 旧版平台文件
│   ├── crawl-works.js                   # 旧版作品爬虫
│   ├── crawl-comments.js                # 旧版评论爬虫
│   └── crawl-direct-messages-v2.js      # 旧版私信爬虫
│
├── global-api-interceptor.js            # ✅ 全局 API 拦截器 (新)
│
├── crawlers/                            # ✨ 新增: 爬虫目录
│   ├── works.js                         # ✅ 作品爬虫 (已完成)
│   ├── comments.js                      # ⏳ 评论爬虫 (待实现)
│   └── messages.js                      # ⏳ 私信爬虫 (待实现)
│
├── utils/                               # ⏳ 工具函数 (待创建)
│   ├── data-merger.js                   # 数据合并工具
│   ├── time-parser.js                   # 时间解析工具
│   └── virtual-list.js                  # 虚拟列表滚动工具
│
└── platform.js                          # ⏳ 平台文件 (需修改集成拦截器)
```

---

## 🎯 已完成模块

### 1. GlobalAPIInterceptor ✅

**文件**: `global-api-interceptor.js` (546 行)

**核心功能**:
```javascript
// 初始化 (在 Platform 中调用)
await globalAPIInterceptor.init(browserContext);

// 获取拦截数据 (在爬虫中调用)
const data = globalAPIInterceptor.getInterceptedData('works');

// 清空缓存 (爬虫任务开始前)
globalAPIInterceptor.clearData('works');

// 查看统计
const stats = globalAPIInterceptor.getStats();
```

**已注册的拦截规则**:
1. ✅ `works-list` - 作品列表 API (`/aweme/v1/creator/item/list/`)
2. ✅ `comments-list` - 评论列表 API (`/comment/.*list/i`)
3. ✅ `discussions-list` - 回复列表 API (`/comment/.*reply/i`)
4. ✅ `conversations-list` - 会话列表 API (`/v1/stranger/get_conversation_list`)
5. ✅ `messages-history` - 消息历史 API (`/v1/stranger/get_message_history`)

---

### 2. 作品爬虫 ✅

**文件**: `crawlers/works.js` (400 行)

**函数导出**:
```javascript
const { crawlWorks } = require('./crawlers/works');

const result = await crawlWorks(page, account, { maxWorks: 100 });
// result = { works: Array, stats: Object }
```

**核心流程**:
1. ✅ 清空 API 缓存 (`globalAPIInterceptor.clearData('works')`)
2. ✅ 导航到作品页 (`creator.douyin.com/creator-micro/content/manage`)
3. ✅ 滚动虚拟列表加载作品 (DOM 提取)
4. ✅ 获取 API 拦截数据
5. ✅ 合并 DOM + API 数据
6. ✅ 标准化并返回

**关键函数**:
- `loadWorksFromVirtualList()` - 虚拟列表滚动
- `extractWorksFromDOM()` - React Fiber 提取
- `mergeWorksData()` - 数据合并
- `standardizeWorkData()` - 标准化格式

---

## ⏳ 待实现模块

### 3. 评论爬虫 (待实现)

**文件**: `crawlers/comments.js` (预计 ~350 行)

**函数签名**:
```javascript
/**
 * 爬取评论和讨论
 * @param {Page} page
 * @param {Object} account
 * @param {Object} options
 * @param {number} [options.maxVideos] - 最多爬取的视频数
 * @returns {Promise<Object>} { comments, discussions, stats }
 */
async function crawlComments(page, account, options = {})
```

**实现步骤**:

```javascript
const globalAPIInterceptor = require('../global-api-interceptor');
const { createLogger } = require('@hiscrm-im/shared/utils/logger');

const logger = createLogger('douyin-crawl-comments');

async function crawlComments(page, account, options = {}) {
  const { maxVideos = null } = options;

  logger.info(`[Account ${account.id}] Starting comments crawl`);

  try {
    // ========================================================================
    // 步骤 1: 清空 API 缓存
    // ========================================================================
    globalAPIInterceptor.clearData('comments');
    globalAPIInterceptor.clearData('discussions');

    // ========================================================================
    // 步骤 2: 导航到评论页
    // ========================================================================
    await page.goto('https://creator.douyin.com/creator-micro/data/comment', {
      waitUntil: 'networkidle',
      timeout: 30000
    });
    await page.waitForTimeout(2000);

    // ========================================================================
    // 步骤 3: 获取视频列表 (DOM)
    // ========================================================================
    const videos = await getVideoList(page);
    logger.info(`Found ${videos.length} videos`);

    // 限制视频数量
    const videosToClick = maxVideos ? videos.slice(0, maxVideos) : videos;

    // ========================================================================
    // 步骤 4: 点击每个视频 (触发 API 请求)
    // ========================================================================
    for (let i = 0; i < videosToClick.length; i++) {
      const video = videosToClick[i];
      logger.info(`Clicking video ${i + 1}/${videosToClick.length}: ${video.title}`);

      await clickVideo(page, video, i);
      await page.waitForTimeout(1000); // 等待 API 响应
    }

    // ========================================================================
    // 步骤 5: 获取 API 拦截数据
    // ========================================================================
    const commentsData = globalAPIInterceptor.getInterceptedData('comments');
    const discussionsData = globalAPIInterceptor.getInterceptedData('discussions');

    logger.info(`Intercepted ${commentsData.length} comment API responses`);
    logger.info(`Intercepted ${discussionsData.length} discussion API responses`);

    // ========================================================================
    // 步骤 6: 解析数据
    // ========================================================================
    const comments = parseComments(commentsData, account);
    const discussions = parseDiscussions(discussionsData, account);

    logger.info(`Parsed ${comments.length} comments`);
    logger.info(`Parsed ${discussions.length} discussions`);

    return {
      comments,
      discussions,
      stats: {
        totalComments: comments.length,
        totalDiscussions: discussions.length,
        videosProcessed: videosToClick.length,
        apiResponses: {
          comments: commentsData.length,
          discussions: discussionsData.length
        }
      }
    };

  } catch (error) {
    logger.error('❌ FATAL ERROR in comments crawl:', error);
    throw error;
  }
}

/**
 * 获取视频列表 (DOM)
 * @private
 */
async function getVideoList(page) {
  // TODO: 实现 DOM 提取视频列表
  // 1. 查找视频列表容器
  // 2. 提取视频标题和索引
  return [];
}

/**
 * 点击视频
 * @private
 */
async function clickVideo(page, video, index) {
  // TODO: 实现点击逻辑
  // 1. 定位视频元素
  // 2. 点击触发评论加载
}

/**
 * 解析评论数据
 * @private
 */
function parseComments(commentsData, account) {
  const allComments = [];

  commentsData.forEach(response => {
    const comments = response.data?.comments || [];

    comments.forEach(c => {
      allComments.push({
        id: uuidv4(),
        account_id: account.id,
        platform: 'douyin',
        platform_user_id: account.platform_user_id,
        platform_comment_id: c.comment_id,

        content: c.text,
        author_name: c.user_info?.screen_name || '匿名',
        author_id: c.user_info?.user_id || '',
        author_avatar: c.user_info?.avatar_url || '',

        // ✅ 新增字段
        is_author: c.is_author || false,
        level: c.level || 1,

        post_id: response.data?.item_id,
        post_title: null, // 需要从 works 表关联

        like_count: c.digg_count || 0,
        reply_count: c.reply_count || 0,

        detected_at: Math.floor(Date.now() / 1000),
        created_at: Math.floor(Date.now() / 1000),

        is_read: false,
        is_new: true,
        push_count: 0
      });
    });
  });

  return allComments;
}

/**
 * 解析讨论数据
 * @private
 */
function parseDiscussions(discussionsData, account) {
  const allDiscussions = [];

  discussionsData.forEach(response => {
    const replies = response.data?.replies || [];
    const parentCommentId = response.data?.comment_id;

    replies.forEach(r => {
      allDiscussions.push({
        id: uuidv4(),
        account_id: account.id,
        platform: 'douyin',
        platform_user_id: account.platform_user_id,
        platform_discussion_id: r.comment_id,

        parent_comment_id: parentCommentId, // 需要从 comments 表关联
        content: r.text,

        author_name: r.user_info?.screen_name || '匿名',
        author_id: r.user_info?.user_id || '',
        author_avatar: r.user_info?.avatar_url || '',

        // ✅ 新增字段
        is_author: r.is_author || false,
        level: r.level || 2,

        reply_to_user_id: r.reply_to_user_info?.user_id || null,
        reply_to_user_name: r.reply_to_user_info?.screen_name || null,
        reply_to_user_avatar: r.reply_to_user_info?.avatar_url || null,

        like_count: r.digg_count || 0,
        reply_count: r.reply_count || 0,

        detected_at: Math.floor(Date.now() / 1000),
        created_at: Math.floor(Date.now() / 1000),

        is_read: false,
        is_new: true,
        push_count: 0
      });
    });
  });

  return allDiscussions;
}

module.exports = {
  crawlComments
};
```

**实现要点**:
1. 视频列表提取 - DOM 解析
2. 点击触发 API - Playwright 点击操作
3. 数据解析 - 从拦截器获取结构化数据
4. parent_comment_id 关联 - 需要建立 comment_id 映射

---

### 4. 私信爬虫 (待实现)

**文件**: `crawlers/messages.js` (预计 ~400 行)

**函数签名**:
```javascript
/**
 * 爬取私信消息
 * @param {Page} page
 * @param {Object} account
 * @param {Object} options
 * @returns {Promise<Object>} { conversations, messages, stats }
 */
async function crawlMessages(page, account, options = {})
```

**实现步骤**:

```javascript
const globalAPIInterceptor = require('../global-api-interceptor');
const { createLogger } = require('@hiscrm-im/shared/utils/logger');

const logger = createLogger('douyin-crawl-messages');

async function crawlMessages(page, account, options = {}) {
  logger.info(`[Account ${account.id}] Starting messages crawl`);

  try {
    // ========================================================================
    // 步骤 1: 清空 API 缓存
    // ========================================================================
    globalAPIInterceptor.clearData('conversations');
    globalAPIInterceptor.clearData('messages');

    // ========================================================================
    // 步骤 2: 导航到私信页
    // ========================================================================
    await page.goto('https://creator.douyin.com/creator-micro/data/following/chat', {
      waitUntil: 'networkidle',
      timeout: 30000
    });
    await page.waitForTimeout(2000);

    // ========================================================================
    // 步骤 3: 获取会话列表
    // ========================================================================
    const conversations = await extractConversations(page, account);
    logger.info(`Found ${conversations.length} conversations`);

    // ========================================================================
    // 步骤 4: 遍历每个会话,获取消息历史
    // ========================================================================
    for (let i = 0; i < conversations.length; i++) {
      const conv = conversations[i];
      logger.info(`Processing conversation ${i + 1}/${conversations.length}: ${conv.platform_user_name}`);

      await openConversation(page, conv, i);
      await scrollMessageHistory(page);
      await returnToConversationList(page);

      await page.waitForTimeout(500);
    }

    // ========================================================================
    // 步骤 5: 获取 API 拦截数据
    // ========================================================================
    const messagesData = globalAPIInterceptor.getInterceptedData('messages');
    logger.info(`Intercepted ${messagesData.length} message API responses`);

    // ========================================================================
    // 步骤 6: 解析消息数据
    // ========================================================================
    const messages = parseMessages(messagesData, conversations, account);
    logger.info(`Parsed ${messages.length} messages`);

    return {
      conversations,
      messages,
      stats: {
        totalConversations: conversations.length,
        totalMessages: messages.length,
        apiResponses: messagesData.length
      }
    };

  } catch (error) {
    logger.error('❌ FATAL ERROR in messages crawl:', error);
    throw error;
  }
}

// ... 实现 extractConversations, openConversation, scrollMessageHistory, parseMessages

module.exports = {
  crawlMessages
};
```

---

### 5. Platform 修改 (待实现)

**文件**: `platform.js` (需修改)

**修改点**:

#### 5.1 引入全局拦截器

```javascript
const globalAPIInterceptor = require('./global-api-interceptor');
const { crawlWorks } = require('./crawlers/works');
const { crawlComments } = require('./crawlers/comments');
const { crawlMessages } = require('./crawlers/messages');
```

#### 5.2 在 login() 中初始化拦截器

```javascript
async login(accountId) {
  // ... 现有登录逻辑 ...

  // 🆕 初始化全局拦截器
  const context = this.browserManager.getContext(accountId);
  if (context && !globalAPIInterceptor.isReady()) {
    await globalAPIInterceptor.init(context);
    logger.info('✅ Global API Interceptor initialized for account', accountId);
  }

  return { success: true };
}
```

#### 5.3 修改 crawlWorks, crawlComments, crawlDirectMessages

```javascript
async crawlWorks(accountId) {
  const page = await this.getPage(accountId);
  const account = await this.getAccountInfo(accountId);

  // 🆕 使用新的爬虫模块
  return await crawlWorks(page, account, { maxWorks: 100 });
}

async crawlComments(accountId) {
  const page = await this.getPage(accountId);
  const account = await this.getAccountInfo(accountId);

  // 🆕 使用新的爬虫模块
  return await crawlComments(page, account, { maxVideos: null });
}

async crawlDirectMessages(accountId) {
  const page = await this.getPage(accountId);
  const account = await this.getAccountInfo(accountId);

  // 🆕 使用新的爬虫模块
  return await crawlMessages(page, account, {});
}
```

---

## 🔧 工具函数 (可选,待创建)

### utils/data-merger.js

```javascript
/**
 * 通用数据合并工具
 */
function mergeByKey(domData, apiData, keyField = 'id') {
  const apiMap = new Map();

  apiData.forEach(response => {
    const items = response.data?.items || [];
    items.forEach(item => {
      const key = item[keyField];
      if (key) apiMap.set(String(key), item);
    });
  });

  return domData.map(dom => {
    const api = apiMap.get(String(dom[keyField]));
    return api ? { ...dom, ...api, source: 'api_enhanced' } : dom;
  });
}

module.exports = { mergeByKey };
```

### utils/time-parser.js

```javascript
/**
 * 时间解析工具
 */
function parseCreatorAPITime(timeValue) {
  if (!timeValue) return null;

  if (typeof timeValue === 'number') {
    return timeValue > 9999999999 ? Math.floor(timeValue / 1000) : timeValue;
  }

  try {
    const date = new Date(timeValue);
    return Math.floor(date.getTime() / 1000);
  } catch {
    return null;
  }
}

function parseMillisecondsTime(timeValue) {
  if (!timeValue) return null;

  const time = parseInt(timeValue);
  if (time > 9999999999) {
    return Math.floor(time / 1000);
  }
  return time;
}

module.exports = {
  parseCreatorAPITime,
  parseMillisecondsTime
};
```

---

## 📊 数据流程对比

### 旧架构 (分散式)

```
crawl-works.js
├─ setupAPIInterceptors()     ← 拦截器逻辑
├─ loadAllWorks()              ← DOM 操作
└─ enhanceWorksWithAPIData()  ← 数据合并

crawl-comments.js
├─ page.on('response')         ← 拦截器逻辑
├─ clickVideos()               ← DOM 操作
└─ parseComments()             ← 数据合并

每个爬虫都重复实现拦截逻辑!
```

### 新架构 (集中式)

```
GlobalAPIInterceptor (全局)
├─ 统一拦截所有 API
├─ 自动匹配规则
└─ 存储结构化数据

crawlers/works.js
├─ loadWorksFromDOM()          ← 仅 DOM 操作
└─ mergeWorksData()            ← 简单合并

crawlers/comments.js
├─ getVideoList()              ← 仅 DOM 操作
├─ clickVideos()               ← 仅 DOM 操作
└─ parseComments()             ← 简单解析

拦截逻辑统一管理,爬虫代码简洁清晰!
```

---

## ✅ 下一步行动

### 立即可做 (按优先级)

1. **实现评论爬虫** (`crawlers/comments.js`)
   - 时间: 2-3 小时
   - 难度: 中
   - 依赖: GlobalAPIInterceptor ✅

2. **实现私信爬虫** (`crawlers/messages.js`)
   - 时间: 2-3 小时
   - 难度: 中
   - 依赖: GlobalAPIInterceptor ✅

3. **修改 Platform** (`platform.js`)
   - 时间: 1-2 小时
   - 难度: 低
   - 依赖: 所有爬虫模块 ⏳

4. **端到端测试**
   - 时间: 2-3 小时
   - 难度: 低
   - 依赖: 全部完成 ⏳

### 总工作量估算

- ✅ 已完成: GlobalAPIInterceptor + works.js (约 6 小时)
- ⏳ 待完成: comments.js + messages.js + platform.js (约 6-8 小时)
- ⏳ 测试和调试: (约 2-3 小时)

**总计**: 14-17 小时 (约 2 个工作日)

---

## 🎯 成功标准

### 功能验证

- [ ] 全局拦截器成功初始化
- [ ] 作品 API 拦截成功 (source = 'api_enhanced')
- [ ] 评论 API 拦截成功 (is_author, level 字段正确)
- [ ] 讨论 API 拦截成功 (reply_to_user_info 字段完整)
- [ ] 私信 API 拦截成功 (conversations + messages)

### 性能验证

- [ ] 内存占用 < 旧架构 (无冗余拦截器)
- [ ] 拦截成功率 > 95%
- [ ] 爬取速度与旧架构相当

### 代码质量

- [ ] 代码行数减少 > 30%
- [ ] 拦截逻辑统一管理
- [ ] 爬虫代码清晰易懂
- [ ] 文档完整

---

**文档维护**: Claude Code
**当前进度**: 基础架构完成 ✅, 评论和私信爬虫待实现 ⏳
**预计完成**: 2025-10-28
